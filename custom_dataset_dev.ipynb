{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7025066a-18b7-4025-a3b7-be209ca383e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import datasets\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dcf41963-877c-4d46-9df3-0c4e22ed284f",
   "metadata": {},
   "outputs": [],
   "source": [
    "B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "B_SYS, E_SYS = \"<<SYS>>\", \"<</SYS>>\"\n",
    "SYSTEM_PROMPT = \"You are a helpful, creative and honest assistant. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\"\n",
    "\n",
    "def tokenize_interaction(tokenizer, instruction, response):\n",
    "    prompt_tokens = tokenizer.encode(f\"{tokenizer.bos_token} {B_INST} {B_SYS}\\n{SYSTEM_PROMPT}\\n{E_SYS}\\n{instruction} {E_INST}\\n\\n\", add_special_tokens=False)\n",
    "    answer_tokens = tokenizer.encode(f\"{response} {tokenizer.eos_token}\", add_special_tokens=False)\n",
    "\n",
    "    interaction_tokens = list(itertools.chain(prompt_tokens, answer_tokens))\n",
    "    labels_tokens = list(itertools.chain(len(prompt_tokens)*[-100,], answer_tokens))\n",
    "\n",
    "    combined_tokens = {\n",
    "        \"input_ids\": interaction_tokens,\n",
    "        \"labels\": labels_tokens,\n",
    "    }\n",
    "\n",
    "    return dict(combined_tokens, attention_mask=[1]*len(combined_tokens[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31dd04bc-bfee-4cc5-8c5a-3b1df4d9a9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_custom_dataset(dataset_config, tokenizer, split):\n",
    "    dataset = Dataset.from_csv(\"instruct_dataset_final.csv\", split=split)\n",
    "\n",
    "    dataset = dataset.map(\n",
    "        lambda sample: {\n",
    "            \"instruction\": sample[\"instruction\"],\n",
    "            \"response\": sample[\"raw_response\"]\n",
    "        },\n",
    "        batched=True,\n",
    "        remove_columns=list(dataset.features)\n",
    "    )\n",
    "\n",
    "    dataset = dataset.map(\n",
    "        lambda x: tokenize_dialog(tokenizer, x[\"instruction\"], x[\"response\"]),\n",
    "        remove_columns=list(dataset.features)\n",
    "    )\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d03c80e0-3a84-4402-abd0-1f34c4adb2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TOK:\n",
    "    def __init__(self):\n",
    "        self.bos_token = \"<s>\"\n",
    "        self.eos_token = \"</s>\"\n",
    "    def encode(self, arg, add_special_tokens):\n",
    "        return arg.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9328140a-f003-495e-a585-90e686511e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TOK()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fe95de07-3b7c-4d54-9036-b9bbd2d0a160",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': ['<s>',\n",
       "  '[INST]',\n",
       "  '<<SYS>>\\nYou',\n",
       "  'are',\n",
       "  'a',\n",
       "  'helpful,',\n",
       "  'creative',\n",
       "  'and',\n",
       "  'honest',\n",
       "  'assistant.',\n",
       "  'Please',\n",
       "  'ensure',\n",
       "  'that',\n",
       "  'your',\n",
       "  'responses',\n",
       "  'are',\n",
       "  'socially',\n",
       "  'unbiased',\n",
       "  'and',\n",
       "  'positive',\n",
       "  'in',\n",
       "  'nature.',\n",
       "  'If',\n",
       "  'a',\n",
       "  'question',\n",
       "  'does',\n",
       "  'not',\n",
       "  'make',\n",
       "  'any',\n",
       "  'sense,',\n",
       "  'or',\n",
       "  'is',\n",
       "  'not',\n",
       "  'factually',\n",
       "  'coherent,',\n",
       "  'explain',\n",
       "  'why',\n",
       "  'instead',\n",
       "  'of',\n",
       "  'answering',\n",
       "  'something',\n",
       "  'not',\n",
       "  'correct.',\n",
       "  'If',\n",
       "  'you',\n",
       "  \"don't\",\n",
       "  'know',\n",
       "  'the',\n",
       "  'answer',\n",
       "  'to',\n",
       "  'a',\n",
       "  'question,',\n",
       "  'please',\n",
       "  \"don't\",\n",
       "  'share',\n",
       "  'false',\n",
       "  'information.\\n<</SYS>>\\ndo',\n",
       "  'it',\n",
       "  '[/INST]\\n\\n',\n",
       "  'okay',\n",
       "  'done',\n",
       "  '</s>'],\n",
       " 'labels': [-100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  'okay',\n",
       "  'done',\n",
       "  '</s>'],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_interaction(tokenizer, \"do it\", \"okay done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a7509e-f2d3-47e8-bee1-f62af7735f42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda",
   "language": "python",
   "name": "conda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
